---
title: "2023-7-20-Blog-Post-4"
author: "Demetrios Samaras"
date: "2023-07-20"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, message = FALSE)

```

```{r render , echo=FALSE, eval=FALSE}
rmarkdown::render("C:/Users/Demetri/Documents/NCSU_masters/ST558/Repos/demetriossamaras.github.io/_Rmd/2023-7-20-Blog-Post-4.Rmd", output_format = "github_document" , output_dir = "../_posts", output_options = list(html_preview = FALSE, keep_html = FALSE))
```


We just finished our section on machine learning (woot).  What method did you find most interesting?  Write a little about the method, fit the model on some data, and provide any relevant output. 

The method that I found the most interesting was boosted tree (partially because it is much quicker to run than random forest). This method essentially created a predictive model using decisions trees iteratively where each iteration is improved by the performance of the previous trees. An example of the code to fit tuning parameters and run the prediction with a training and test set using the iris data set is below.   

```{r library, echo=FALSE, eval=TRUE}
library(tidyverse)
library(caret)
library(knitr)
library(gbm)
library(tree)
```


```{r boosted tree }
set.seed(90)
## creates a index of train and test variables 
train <- sample(1:nrow(iris), size = nrow(iris)*.80)
test <- setdiff(1:nrow(iris), train)

## selects the train and test rows 
iris_train <- iris[train, ]
iris_test <- iris[test, ]

## creates grid of possible tuning parameters 
gbm_grid <-  expand.grid(interaction.depth = c(1,4,7), 
  n.trees = c(1:20) , 
  shrinkage = 0.1,
  n.minobsinnode = c(20))

## sets trainControl method 
fit_control <- trainControl(method = "repeatedcv",
                            number = 5,
                            repeats= 1)

set.seed(13)

## trains to find optimal tuning parameters for species based on all other variables  
gbm_tree_cv <- train(Species ~ . , data = iris_train,
                     method = "gbm",
                     preProcess = c("center", "scale"),
                     trControl = fit_control,
                     tuneGrid= gbm_grid,
                     verbose=FALSE)
## plot to visualize parameters 
plot(gbm_tree_cv)

## test set prediction
boosted_tree_model_pred <- predict(gbm_tree_cv, newdata = dplyr::select(iris_test, -Species))

## stores results 
boosted_tree_ACC <- postResample(boosted_tree_model_pred, obs = iris_test$Species)

boosted_tree_ACC